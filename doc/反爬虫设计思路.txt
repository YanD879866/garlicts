简单思路：

1、有些爬虫初学者写的爬虫，对网站的破坏性特别大，所以这类进行第一道过滤。
user-agent过滤、1秒内访问次数超过2次。
缺陷：当然有些爬虫会伪造user-agent，会放缓访问频率，此类爬虫过滤不了。

2、对于非游览器进行第二道过滤。
页面加载时，进行ajax请求后台IP(IP + cookie)统计，在处理请求时，发现统计中没有这个IP信息，则判定为非游览器，过滤掉。
缺陷：当然也有些爬虫就是基于无界面游览器的，此类爬虫，过滤不了。

3、对于爬虫伪装为搜索引擎的，进行第三道过滤。
获取user-agent和ip，通过linux的host命令或ip白名单对当前的ip进行真实性验证。

4、连续访问超过5个页面的ip，进行第四道过滤
连续访问超过5个页面，弹出验证码，输入验证码，才可以继续查看页面。只有真实的客户，才会输入验证码。
缺陷：对用户的体验有一定影响。








